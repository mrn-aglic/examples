images:
  airflow:
    repository: apache/airflow:2.4.2 #-python3.10
    tag: 2.4.2-python3.10
    pullPolicy: IfNotPresent


executor: LocalKubernetesExecutor

config:
  webserver:
    expose_config: 'True'


#kubectl create secret generic airflow-ch18-env --from-env-file .env -n airflow
#  - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@postgres:5432/airflow
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  - AIRFLOW__KUBERNETES__IN_CLUSTER=False
  - AIRFLOW_CONN_WILDFIRES_API=http://wildfires-api:8000

logs:
  persistence:
    enabled: true
    size: 2Gi

webserver:
  # Create initial user.
  defaultUser:
    enabled: true
    role: Admin
    username: airflow
    password: airflow


triggerer:
  enabled: false

redis:
  enabled: false

statsd:
  enabled: false

cleanup:
  enabled: false

migrateDatabaseJob:
  enabled: false


dags:
  gitSync:
    enabled: true
    repo: https://github.com/mrn-aglic/examples.git # or replace with your github repo
    branch: master
    subPath: "airflow-new-features/dags/feat23/dags"
    sshKeySecret: airflow-git-secret

# I believe that this is not required, hence it is commented out.
#env:
#  - name: GOOGLE_APPLICATION_CREDENTIALS
#    value: "/opt/airflow/secrets/key.json"

#secret:
#  - envName: RATINGS_BUCKET
#    secretName: airflow-ch18-env
#    secretKey: RATINGS_BUCKET
#  - envName: RESULT_BUCKET
#    secretName: airflow-ch18-env
#    secretKey: RESULT_BUCKET
#  - envName: BIGQUERY_DATASET
#    secretName: airflow-ch18-env
#    secretKey: BIGQUERY_DATASET
#  - envName: GCP_PROJECT
#    secretName: airflow-ch18-env
#    secretKey: GCP_PROJECT
#  - envName: MOVIELENS_USER
#    secretName: airflow-ch18-env
#    secretKey: MOVIELENS_USER
#  - envName: MOVIELENS_PASSWORD
#    secretName: airflow-ch18-env
#    secretKey: MOVIELENS_PASSWORD
